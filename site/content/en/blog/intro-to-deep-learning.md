---
title: "Introduction to Deep Learning and Artificial Neural Networks"
date: 2023-07-23T20:00:00Z
draft: false
tags: ["AI"]
thumbnail: "https://j-santana-dev.github.io/itguynextdoor.github.io/pietro-jeng-n6B49lTx7NM-unsplash.jpg"
description: "In this post, I will focus on one of the most popular subfields of AI: Deep Learning (DL)."
---

## Introduction
In my [previous post]({{< ref "/blog/welcome-to-the-ai-world" >}}), I talked about some basics aspects of AI. We saw that AI is a broad field and it is divided into many subfields. In this post, I will focus on one of the most popular subfields of AI: **Deep Learning (DL)**.

Deep Learning is a specialized area within Machine Learning (ML) that aims to replicate the intricate way in which the human brain works. Deep Learning finds extensive application in many fields such as computer vision, natural language processing, speech recognition, and many more. 

In this post, I will give you a brief introduction to Deep Learning and Artificial Neural Networks (ANNs). I will also talk about the different types of ANNs and their use cases.

## So... What is the aim of Deep Learning?
Deep Learning focuses on training **Artificial Neural Networks (ANNs)** using large amounts of unstructured data. It employs algorithms inspired by the neural networks of the human brain with the main goal of extracting patterns from the data.

For example, let's say that you want to build a system that can recognize images of bees. You can use Deep Learning to train an ANN to perform this task. You can feed the ANN with terabytes of images of bees. The ANN will learn how to recognize bees by extracting patterns from the images. Once the ANN is trained, you can use it to recognize a bee in a new image. 

## Okay... but, what is an Artificial Neural Network then?
An Artificial Neural Network (ANN) is a computational model inspired by the human brain and is the foundation of Deep Learning algorithms. ANNs consist of interconnected nodes, called "neurons", organized into multiple layers. Each layer is responsible for identifying specific features or patterns in the input data. The first layer is called the "input layer" and the last layer is called the "output layer". The layers in between are called "hidden layers". The "depth" of an ANN is the number of hidden layers it has.

For example, in the image below, you can see a simple ANN with three layers: an input layer, a hidden layer, and an output layer. The input layer has three neurons, the hidden layer has four neurons, and the output layer has two neurons. This ANN has a depth of one because it has only one hidden layer.

![Artificial Neural Network](https://j-santana-dev.github.io/itguynextdoor.github.io/ann-wiki.png)

The arrows in the diagram represent the connections between neurons. Each connection has a **weight** associated with it, and **these weights are learned during the training process**. The neurons in the hidden layer are activated based on the weighted sum of inputs from the previous layer. The output of the hidden layer is then passed to the output layer. The output layer, in turn, is activated by the weighted sum of inputs from the hidden layer. The output generated by the output layer represents the final output of the ANN.

## Why are layers important?
Layers in an Artificial Neural Network (ANN) play a crucial role in extracting features from the input data. Each layer focuses on identifying different levels of features. The initial layer extracts simple or basic features, while subsequent layers extract more complex and abstract features. By adding more layers to an ANN, it becomes capable of recognizing intricate and detailed features from the input data.

It's important to note that the relationship between the number of neurons per layer and the number of features is not a direct one-to-one mapping. The number of neurons in a layer is influenced by factors such as the complexity of the problem, desired model capacity, and architectural considerations. Choosing the optimal number of neurons per layer often involves experimentation.

## How does an ANN learn?
The learning process of an Artificial Neural Network is known as "training". During the training process, the ANN "learns" how to perform a specific task by adjusting its weights. The training process is an iterative process that consists of the following steps:

1. The ANN is provided with a training sample (input data). It is important to note that, before feeding the sample to the ANN, the data must be converted into a format that the ANN can understand, that is, a **tensor**.
2. The ANN processes the input data and produces an output by passing the information through its layers. This step, known as "forward propagation", involves computations and activations within the network. Under the hood, the ANN performs mathematical operations, such as matrix multiplications, to generate the output.
3. The output produced by the ANN is compared to the expected output. This step measures the error or the difference between the predicted output and the desired output.
4. The error is propagated back through the ANN, adjusting the weights in a process called "backpropagation". This step aims to minimize the error by updating the weights using an optimization algorithm called "gradient descent".
5. Steps 1-4 are repeated until the error is "small enough", that is, until the error reaches an acceptable or predefined threshold. This indicates that the ANN has learned to perform the task effectively. The training phase can take a considerable amount of time, depending on the complexity of the task and the available data.
6. Once the ANN is trained, it can be used in the "inference" phase. During inference, the network is given new data, and it generates predictions or outputs based on its learned knowledge. The output is typically represented in the form of a tensor and may be converted into a format that is easily understandable.

You might have noticed that I mentioned "tensors" or "matrix multiplications" in the previous steps. This is because ANNs can only process tensors, which are N-dimensional arrays. Tensors are fundamental in Deep Learning and are used to represent various types of data, such as images, sound, and text, in a numerical format. So, when delving into the world of ANNs, you'll find tensors throughout the learning process!

![tensor-always-has-been](https://j-santana-dev.github.io/itguynextdoor.github.io/tensor-always-has-been.jpg)

## Classification of Neural Networks
There are many types of Neural Networks, each designed to solve specific tasks. The table below presents some popular types of ANNs along with their primary characteristics and typical use cases.

| Neural Network Type            | Main Characteristics                     | Typical Use Cases                                    |
|-------------------------------|------------------------------------------|------------------------------------------------------|
| Feedforward Neural Networks   | Unidirectional, used for classification, regression, pattern recognition.               | Image classification, sentiment analysis.            |
| Convolutional Neural Networks | Process grid-like data (images), extract local features.  | Object detection, computer vision tasks.             |
| Recurrent Neural Networks     | Capture sequential information, allow feedback connections.  | Natural language processing, speech recognition.     |
| Long Short-Term Memory Networks  | Specialized RNNs, handle long-term dependencies.    | Language translation, speech recognition.           |
| Generative Adversarial Networks  | Composed of generator and discriminator networks, generate new data. | Image synthesis, generative modeling.             |
| Autoencoders                  | Compress input data into latent representation, reconstruction.  | Dimensionality reduction, anomaly detection.        |

This brief overview of the main types of Neural Networks aims to provide you with an understanding of their diversity. If you want to explore each type further, you can find more detailed information online :smile:

## Which use cases are suitable for Deep Learning, and which ones are not?
Deep Learning is great for:
* Solving problems with a long list of rules.
* Discovering patterns within large collections of unstructured data (images, text, audio, etc.).
* Problems with continuous changes in the environment.

Deep Learning is not so great for:
* Problems with small datasets. Models need large amounts of data to perform well.
* When you need explainability. Deep Learning models are black boxes and they are often impossible to understand by humans.
* When errors are totally unacceptable. Deep Learning models are probabilistic and they are not 100% accurate.

## Conclusion
In conclusion, Deep Learning (DL), a subfield of AI, offers a powerful approach to solving complex problems by training Artificial Neural Networks (ANNs) with huge amounts of unstructured data. ANNs, inspired by the human brain, consist of interconnected nodes organized into layers. By adjusting weights and propagating errors, ANNs learn to extract features and patterns from data, enabling tasks such as image recognition and natural language processing. However, Deep Learning is not suitable for small datasets, lacks explainability, and has inherent probabilistic limitations.

## What's next?
In the next post, I'm going to talk about PyTorch, a popular Deep Learning framework. I'll show you how to build a simple ANN and how to train it to perform a simple task. Stay tuned!

---

![Would you like to know more?](https://j-santana-dev.github.io/itguynextdoor.github.io/know-more.png)

For a more visual understanding of Artificial Neural Networks (ANNs) you can check out the video below. It provides a great overview of ANNs and their basic working principles.

{{< youtube aircAruvnKk >}}